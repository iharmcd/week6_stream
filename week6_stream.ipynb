{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94fa204a-706a-455e-9e48-5da1fcab0d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time \n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c83c9cb-e0a5-4c72-bc6e-14f903fc0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "topic_name = 'test-topic'\n",
    "\n",
    "for i in range(10):\n",
    "    message = {'number': i}\n",
    "    producer.send(topic_name, value=message)\n",
    "    print(f\"Sent: {message}\")\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'Message took {(t1 - t0):.2f} seconds')\n",
    "\n",
    "producer.flush()\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print(f'Flush took {(t2 - t1):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99dcc96-b69f-4df9-8cfd-a96a1dcdaa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ccea69-52aa-4840-8315-4cd1356edb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5911447-9e22-4d50-b4fc-27982159bd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66416/2666272317.py:9: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, compression='gzip')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>tip_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-01 00:26:02</td>\n",
       "      <td>2019-10-01 00:39:58</td>\n",
       "      <td>112</td>\n",
       "      <td>196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-01 00:18:11</td>\n",
       "      <td>2019-10-01 00:22:38</td>\n",
       "      <td>43</td>\n",
       "      <td>263</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-01 00:09:31</td>\n",
       "      <td>2019-10-01 00:24:47</td>\n",
       "      <td>255</td>\n",
       "      <td>228</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-01 00:37:40</td>\n",
       "      <td>2019-10-01 00:41:49</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-01 00:08:13</td>\n",
       "      <td>2019-10-01 00:17:56</td>\n",
       "      <td>97</td>\n",
       "      <td>188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lpep_pickup_datetime lpep_dropoff_datetime  PULocationID  DOLocationID  \\\n",
       "0  2019-10-01 00:26:02   2019-10-01 00:39:58           112           196   \n",
       "1  2019-10-01 00:18:11   2019-10-01 00:22:38            43           263   \n",
       "2  2019-10-01 00:09:31   2019-10-01 00:24:47           255           228   \n",
       "3  2019-10-01 00:37:40   2019-10-01 00:41:49           181           181   \n",
       "4  2019-10-01 00:08:13   2019-10-01 00:17:56            97           188   \n",
       "\n",
       "   passenger_count  trip_distance  tip_amount  \n",
       "0              1.0           5.88        0.00  \n",
       "1              1.0           0.80        0.00  \n",
       "2              2.0           7.50        0.00  \n",
       "3              1.0           0.90        0.00  \n",
       "4              1.0           2.52        2.26  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'green_tripdata_2019-10.csv.gz'\n",
    "\n",
    "dtype_options = {'store_and_fwd_flag': 'string', 'lpep_dropoff_datetime': 'string'}\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path, compression='gzip')\n",
    "\n",
    "df = df[['lpep_pickup_datetime',\n",
    "'lpep_dropoff_datetime',\n",
    "'PULocationID',\n",
    "'DOLocationID',\n",
    "'passenger_count',\n",
    "'trip_distance',\n",
    "'tip_amount']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454e106-44d3-42cf-8552-3e8f822935bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea7041-b2ed-4846-a14a-90adda6e97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.itertuples(index=False):\n",
    "    row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "    print(row_dict)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a1c5b9-4f84-45fe-b5c3-21aa7fffabca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message took 101.52 seconds\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "topic_name = 'green-trips'\n",
    "t0 = time.time()\n",
    "\n",
    "for row in df.itertuples(index=False):\n",
    "    row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "    producer.send(topic_name, value=row_dict)\n",
    "    \n",
    "\n",
    "producer.flush()\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'Message took {(t1 - t0):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a5162-39e6-4ac7-920a-6ba995664463",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27294a0a-7e65-4767-acbf-10abe37c72a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 23:20:18 WARN Utils: Your hostname, codespaces-a6a29b resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "24/03/17 23:20:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/python/3.10.13/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/codespace/.ivy2/cache\n",
      "The jars for the packages stored in: /home/codespace/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-830c3220-55e1-4e64-bacd-d51fd2a2d583;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 871ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-830c3220-55e1-4e64-bacd-d51fd2a2d583\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/17ms)\n",
      "24/03/17 23:20:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GreenTripsConsumer\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8ac38cb-7b75-44ae-a31e-c07f51ff58ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 23:20:32 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1640f-5f02-48e2-b1ef-f359e30d9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek(mini_batch, batch_id):\n",
    "    first_row = mini_batch.take(1)\n",
    "\n",
    "    if first_row:\n",
    "        print(first_row[0])\n",
    "\n",
    "query = green_stream.writeStream.foreachBatch(peek).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762036b7-5164-4032-9d75-c7671484b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8933e1de-93f7-410f-b902-29919abc528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType() \\\n",
    "    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n",
    "    .add(\"PULocationID\", types.IntegerType()) \\\n",
    "    .add(\"DOLocationID\", types.IntegerType()) \\\n",
    "    .add(\"passenger_count\", types.DoubleType()) \\\n",
    "    .add(\"trip_distance\", types.DoubleType()) \\\n",
    "    .add(\"tip_amount\", types.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "379183af-8bef-408a-af42-dab55ea07fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "green_stream = green_stream \\\n",
    "  .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49295465-0d67-4c83-89f1-499af3c291a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 23:20:49 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-86a539a7-f538-448d-87c4-11d34146417d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/17 23:20:49 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/03/17 23:20:50 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(lpep_pickup_datetime='2019-10-01 00:26:02', lpep_dropoff_datetime='2019-10-01 00:39:58', PULocationID=112, DOLocationID=196, passenger_count=1.0, trip_distance=5.88, tip_amount=0.0)\n"
     ]
    }
   ],
   "source": [
    "def peek(mini_batch, batch_id):\n",
    "    first_row = mini_batch.take(5)\n",
    "\n",
    "    if first_row:\n",
    "        print(first_row[0])\n",
    "\n",
    "query = green_stream.writeStream.foreachBatch(peek).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b875839f-6965-4794-a650-f6b8d6a35bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a440d73c-3fa9-4483-aa99-ee3d148f2b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 23:19:06 WARN Utils: Your hostname, codespaces-a6a29b resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "24/03/17 23:19:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/python/3.10.13/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/codespace/.ivy2/cache\n",
      "The jars for the packages stored in: /home/codespace/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e77d1bc8-4c27-4d19-af09-1576f7e0e4d4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 937ms :: artifacts dl 25ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e77d1bc8-4c27-4d19-af09-1576f7e0e4d4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/19ms)\n",
      "24/03/17 23:19:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/17 23:19:12 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ad015ccb-566c-48c6-904e-b705bd6c5c22. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/17 23:19:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/03/17 23:19:13 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                                                                                                                      |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:26:02\", \"lpep_dropoff_datetime\": \"2019-10-01 00:39:58\", \"PULocationID\": 112, \"DOLocationID\": 196, \"passenger_count\": 1.0, \"trip_distance\": 5.88, \"tip_amount\": 0.0}|\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:18:11\", \"lpep_dropoff_datetime\": \"2019-10-01 00:22:38\", \"PULocationID\": 43, \"DOLocationID\": 263, \"passenger_count\": 1.0, \"trip_distance\": 0.8, \"tip_amount\": 0.0}  |\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:09:31\", \"lpep_dropoff_datetime\": \"2019-10-01 00:24:47\", \"PULocationID\": 255, \"DOLocationID\": 228, \"passenger_count\": 2.0, \"trip_distance\": 7.5, \"tip_amount\": 0.0} |\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:37:40\", \"lpep_dropoff_datetime\": \"2019-10-01 00:41:49\", \"PULocationID\": 181, \"DOLocationID\": 181, \"passenger_count\": 1.0, \"trip_distance\": 0.9, \"tip_amount\": 0.0} |\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:08:13\", \"lpep_dropoff_datetime\": \"2019-10-01 00:17:56\", \"PULocationID\": 97, \"DOLocationID\": 188, \"passenger_count\": 1.0, \"trip_distance\": 2.52, \"tip_amount\": 2.26}|\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:35:01\", \"lpep_dropoff_datetime\": \"2019-10-01 00:43:40\", \"PULocationID\": 65, \"DOLocationID\": 49, \"passenger_count\": 1.0, \"trip_distance\": 1.47, \"tip_amount\": 1.86} |\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:28:09\", \"lpep_dropoff_datetime\": \"2019-10-01 00:30:49\", \"PULocationID\": 7, \"DOLocationID\": 179, \"passenger_count\": 1.0, \"trip_distance\": 0.6, \"tip_amount\": 1.0}   |\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:28:26\", \"lpep_dropoff_datetime\": \"2019-10-01 00:32:01\", \"PULocationID\": 41, \"DOLocationID\": 74, \"passenger_count\": 1.0, \"trip_distance\": 0.56, \"tip_amount\": 0.0}  |\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:14:01\", \"lpep_dropoff_datetime\": \"2019-10-01 00:26:16\", \"PULocationID\": 255, \"DOLocationID\": 49, \"passenger_count\": 1.0, \"trip_distance\": 2.42, \"tip_amount\": 0.0} |\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:03:03\", \"lpep_dropoff_datetime\": \"2019-10-01 00:17:13\", \"PULocationID\": 130, \"DOLocationID\": 131, \"passenger_count\": 1.0, \"trip_distance\": 3.4, \"tip_amount\": 2.85}|\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:07:10\", \"lpep_dropoff_datetime\": \"2019-10-01 00:23:38\", \"PULocationID\": 24, \"DOLocationID\": 74, \"passenger_count\": 3.0, \"trip_distance\": 3.18, \"tip_amount\": 0.0}  |\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:25:48\", \"lpep_dropoff_datetime\": \"2019-10-01 00:49:52\", \"PULocationID\": 255, \"DOLocationID\": 188, \"passenger_count\": 1.0, \"trip_distance\": 4.7, \"tip_amount\": 1.0} |\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:03:12\", \"lpep_dropoff_datetime\": \"2019-10-01 00:14:43\", \"PULocationID\": 129, \"DOLocationID\": 160, \"passenger_count\": 1.0, \"trip_distance\": 3.1, \"tip_amount\": 0.0} |\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:44:56\", \"lpep_dropoff_datetime\": \"2019-10-01 00:51:06\", \"PULocationID\": 18, \"DOLocationID\": 169, \"passenger_count\": 1.0, \"trip_distance\": 1.19, \"tip_amount\": 0.25}|\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:55:14\", \"lpep_dropoff_datetime\": \"2019-10-01 01:00:49\", \"PULocationID\": 223, \"DOLocationID\": 7, \"passenger_count\": 1.0, \"trip_distance\": 1.09, \"tip_amount\": 1.46} |\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:06:06\", \"lpep_dropoff_datetime\": \"2019-10-01 00:11:05\", \"PULocationID\": 75, \"DOLocationID\": 262, \"passenger_count\": 1.0, \"trip_distance\": 1.24, \"tip_amount\": 2.01}|\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:00:19\", \"lpep_dropoff_datetime\": \"2019-10-01 00:14:32\", \"PULocationID\": 97, \"DOLocationID\": 228, \"passenger_count\": 1.0, \"trip_distance\": 3.03, \"tip_amount\": 3.58}|\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:09:31\", \"lpep_dropoff_datetime\": \"2019-10-01 00:20:41\", \"PULocationID\": 41, \"DOLocationID\": 74, \"passenger_count\": 1.0, \"trip_distance\": 2.03, \"tip_amount\": 2.16} |\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:30:36\", \"lpep_dropoff_datetime\": \"2019-10-01 00:34:30\", \"PULocationID\": 41, \"DOLocationID\": 42, \"passenger_count\": 1.0, \"trip_distance\": 0.73, \"tip_amount\": 1.26} |\n",
      "|{\"lpep_pickup_datetime\": \"2019-10-01 00:58:32\", \"lpep_dropoff_datetime\": \"2019-10-01 01:05:08\", \"PULocationID\": 41, \"DOLocationID\": 116, \"passenger_count\": 1.0, \"trip_distance\": 1.48, \"tip_amount\": 0.0} |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Batch ID: 0\n",
      "+--------------------+---------------------+------------+------------+---------------+-------------+----------+\n",
      "|lpep_pickup_datetime|lpep_dropoff_datetime|PULocationID|DOLocationID|passenger_count|trip_distance|tip_amount|\n",
      "+--------------------+---------------------+------------+------------+---------------+-------------+----------+\n",
      "|2019-10-01 00:26:02 |2019-10-01 00:39:58  |112         |196         |1.0            |5.88         |0.0       |\n",
      "|2019-10-01 00:18:11 |2019-10-01 00:22:38  |43          |263         |1.0            |0.8          |0.0       |\n",
      "|2019-10-01 00:09:31 |2019-10-01 00:24:47  |255         |228         |2.0            |7.5          |0.0       |\n",
      "|2019-10-01 00:37:40 |2019-10-01 00:41:49  |181         |181         |1.0            |0.9          |0.0       |\n",
      "|2019-10-01 00:08:13 |2019-10-01 00:17:56  |97          |188         |1.0            |2.52         |2.26      |\n",
      "|2019-10-01 00:35:01 |2019-10-01 00:43:40  |65          |49          |1.0            |1.47         |1.86      |\n",
      "|2019-10-01 00:28:09 |2019-10-01 00:30:49  |7           |179         |1.0            |0.6          |1.0       |\n",
      "|2019-10-01 00:28:26 |2019-10-01 00:32:01  |41          |74          |1.0            |0.56         |0.0       |\n",
      "|2019-10-01 00:14:01 |2019-10-01 00:26:16  |255         |49          |1.0            |2.42         |0.0       |\n",
      "|2019-10-01 00:03:03 |2019-10-01 00:17:13  |130         |131         |1.0            |3.4          |2.85      |\n",
      "|2019-10-01 00:07:10 |2019-10-01 00:23:38  |24          |74          |3.0            |3.18         |0.0       |\n",
      "|2019-10-01 00:25:48 |2019-10-01 00:49:52  |255         |188         |1.0            |4.7          |1.0       |\n",
      "|2019-10-01 00:03:12 |2019-10-01 00:14:43  |129         |160         |1.0            |3.1          |0.0       |\n",
      "|2019-10-01 00:44:56 |2019-10-01 00:51:06  |18          |169         |1.0            |1.19         |0.25      |\n",
      "|2019-10-01 00:55:14 |2019-10-01 01:00:49  |223         |7           |1.0            |1.09         |1.46      |\n",
      "|2019-10-01 00:06:06 |2019-10-01 00:11:05  |75          |262         |1.0            |1.24         |2.01      |\n",
      "|2019-10-01 00:00:19 |2019-10-01 00:14:32  |97          |228         |1.0            |3.03         |3.58      |\n",
      "|2019-10-01 00:09:31 |2019-10-01 00:20:41  |41          |74          |1.0            |2.03         |2.16      |\n",
      "|2019-10-01 00:30:36 |2019-10-01 00:34:30  |41          |42          |1.0            |0.73         |1.26      |\n",
      "|2019-10-01 00:58:32 |2019-10-01 01:05:08  |41          |116         |1.0            |1.48         |0.0       |\n",
      "+--------------------+---------------------+------------+------------+---------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 23:19:20 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "schema = types.StructType([\n",
    "    types.StructField('lpep_pickup_datetime', types.StringType(), True),\n",
    "    types.StructField('lpep_dropoff_datetime', types.StringType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),    \n",
    "    types.StructField('passenger_count', types.DoubleType(), True),\n",
    "    types.StructField('trip_distance', types.DoubleType(), True),\n",
    "    types.StructField('tip_amount', types.DoubleType(), True)\n",
    "])\n",
    "\n",
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('GreenTripsConsumer').\\\n",
    "    config(\"spark.jars.packages\", kafka_jar_package).getOrCreate()\n",
    "\n",
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "    .load()\n",
    "\n",
    "def debug_and_process(batch_df, batch_id):\n",
    "    batch_df.select(F.col(\"value\").cast('STRING')).show(truncate=False)\n",
    "    parsed_batch = batch_df.select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "    \n",
    "    print(f\"Batch ID: {batch_id}\")\n",
    "    parsed_batch.show(truncate=False)\n",
    "\n",
    "query = green_stream \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(debug_and_process) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b078455-dfaf-4a56-a924-cfe60d53fe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 23:37:11 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-3566c6e3-b813-4f58-a2d8-75cf1c5b6ac6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/17 23:37:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/03/17 23:37:11 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+------------+-----+\n",
      "|              window|DOLocationID|count|\n",
      "+--------------------+------------+-----+\n",
      "|{2024-03-17 23:35...|          74|17741|\n",
      "|{2024-03-17 23:35...|          42|15942|\n",
      "|{2024-03-17 23:35...|          41|14061|\n",
      "|{2024-03-17 23:35...|          75|12840|\n",
      "|{2024-03-17 23:35...|         129|11930|\n",
      "|{2024-03-17 23:35...|           7|11533|\n",
      "|{2024-03-17 23:35...|         166|10845|\n",
      "|{2024-03-17 23:35...|         236| 7913|\n",
      "|{2024-03-17 23:35...|         223| 7542|\n",
      "|{2024-03-17 23:35...|         238| 7318|\n",
      "|{2024-03-17 23:35...|          82| 7292|\n",
      "|{2024-03-17 23:35...|         181| 7282|\n",
      "|{2024-03-17 23:35...|          95| 7244|\n",
      "|{2024-03-17 23:35...|         244| 6733|\n",
      "|{2024-03-17 23:35...|          61| 6606|\n",
      "|{2024-03-17 23:35...|         116| 6339|\n",
      "|{2024-03-17 23:35...|         138| 6144|\n",
      "|{2024-03-17 23:35...|          97| 6050|\n",
      "|{2024-03-17 23:35...|          49| 5221|\n",
      "|{2024-03-17 23:35...|         151| 5153|\n",
      "+--------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/local/python/3.10.13/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     14\u001b[0m query \u001b[38;5;241m=\u001b[39m ordered_stream_df \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Wait for the streaming query to terminate\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col, current_timestamp, window\n",
    "\n",
    "stream_with_timestamp_df = green_stream.withColumn(\"timestamp\", current_timestamp())\n",
    "\n",
    "windowed_stream_df = stream_with_timestamp_df \\\n",
    "    .groupBy(window(col(\"timestamp\"), \"5 minutes\"), col(\"DOLocationID\")) \\\n",
    "    .count()\n",
    "\n",
    "ordered_stream_df = windowed_stream_df.orderBy(col(\"count\").desc())\n",
    "\n",
    "query = ordered_stream_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a9d60-b3e3-47ec-98a7-d25bfee8a817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
